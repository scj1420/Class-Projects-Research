{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game  0\n",
      "Training Time:  29.313282251358032\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import time\n",
    "import random\n",
    "\n",
    "\n",
    "#The following block is where the ANN is actually built.\n",
    "#The one I'm using has only one hidden layer of 20 neurons.\n",
    "network = Sequential()\n",
    "network.add(Dense(40, init='normal', input_dim=9))\n",
    "network.add(Activation('relu'))\n",
    "#network.add(Dense(50, init='normal'))\n",
    "#network.add(Activation('relu'))\n",
    "network.add(Dense(9, init='normal'))\n",
    "network.add(Activation('sigmoid'))\n",
    "network.compile(loss='mse', optimizer='sgd')\n",
    "\n",
    "#This function takes the board and chooses randomly among the spots that are not taken.\n",
    "def randChoice(board):\n",
    "    while True:\n",
    "        rand_spot = np.random.randint(0,9)\n",
    "        if board[rand_spot]==0:\n",
    "            break\n",
    "    return rand_spot\n",
    "\n",
    "#This function creates an expected output array with negative values in all spots that \n",
    "#are already full.\n",
    "def training_board(inboard, output):\n",
    "    board_train = output\n",
    "    for i in range(9):\n",
    "        if inboard[i]==1 or inboard[i]==-1:\n",
    "            board_train[0][i] = -90\n",
    "    return board_train\n",
    "\n",
    "#This function takes the input board and the chosen spot as inputs.\n",
    "#It determines whether the following state is a win or loss or illegal and give positive or negative rewards.\n",
    "def rewardFunction(board, choice, x=True):\n",
    "    if board[choice]==1 or board[choice]==-1:\n",
    "        return -90\n",
    "    new_board = board\n",
    "    if x:\n",
    "        new_board[choice] = 1\n",
    "    else:\n",
    "        new_board[choice] = -1\n",
    "    if (new_board[0]==1 and new_board[1]==1 and new_board[2]==1) or \\\n",
    "    (new_board[3]==1 and new_board[4]==1 and new_board[5]==1) or \\\n",
    "    (new_board[6]==1 and new_board[7]==1 and new_board[8]==1) or \\\n",
    "    (new_board[0]==1 and new_board[3]==1 and new_board[6]==1) or \\\n",
    "    (new_board[1]==1 and new_board[4]==1 and new_board[7]==1) or \\\n",
    "    (new_board[2]==1 and new_board[5]==1 and new_board[8]==1) or \\\n",
    "    (new_board[0]==1 and new_board[4]==1 and new_board[8]==1) or \\\n",
    "    (new_board[2]==1 and new_board[4]==1 and new_board[6]==1):\n",
    "        return 100\n",
    "    if (new_board[0]==-1 and new_board[1]==-1 and new_board[2]==-1) or \\\n",
    "    (new_board[3]==-1 and new_board[4]==-1 and new_board[5]==-1) or \\\n",
    "    (new_board[6]==-1 and new_board[7]==-1 and new_board[8]==-1) or \\\n",
    "    (new_board[0]==-1 and new_board[3]==-1 and new_board[6]==-1) or \\\n",
    "    (new_board[1]==-1 and new_board[4]==-1 and new_board[7]==-1) or \\\n",
    "    (new_board[2]==-1 and new_board[5]==-1 and new_board[8]==-1) or \\\n",
    "    (new_board[0]==-1 and new_board[4]==-1 and new_board[8]==-1) or \\\n",
    "    (new_board[2]==-1 and new_board[4]==-1 and new_board[6]==-1):\n",
    "        return -100\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#This function trains the neural network to play tic tac toe.\n",
    "def trainNetwork(games, gamma, ep):\n",
    "    #All of the training is contained in this loop which iterates according to number of games entered.\n",
    "    for i in range(games):\n",
    "        if i%10000==0:\n",
    "            print('game ', i)\n",
    "        \n",
    "        #The play board is initialized here\n",
    "        board = np.zeros((9,1))\n",
    "        play = True #if someone wins, the loop is broken\n",
    "        j = 0 # this variable is used to ensure only nine total moves\n",
    "        \n",
    "        #This while loop contains on complete game. Broken when game is finished.\n",
    "        while play==True and j < 5:\n",
    "            innum1 = board\n",
    "            \n",
    "            #ANN picks the highest output value with probability 1 - ep. Otherwise is chooses randomly.\n",
    "            outnum1 = network.predict(innum1.reshape(1,9), batch_size=1)\n",
    "            if random.random()>ep:\n",
    "                move_choice = np.argmax(outnum1)\n",
    "            else:\n",
    "                move_choice = randChoice(board)\n",
    "                \n",
    "            #if i>30000 and i%1000==0:\n",
    "                #print('Input: \\n', innum1)\n",
    "                #print('Output: \\n', outnum1)\n",
    "            \n",
    "            #Reward is calculated for whichever choice\n",
    "            reward = rewardFunction(innum1, move_choice)\n",
    "            \n",
    "            #Here a random unoccupied spot is chosen if the NN outputs the highest value at a taken spot.\n",
    "            if reward==-90:\n",
    "                move_choice = randChoice(board)\n",
    "                reward = rewardFunction(innum1, move_choice)\n",
    "            \n",
    "            #Board spot is filled with a 1, which is an X\n",
    "            board[move_choice] = 1\n",
    "            \n",
    "            #If the X's win, the neural networks is trained to predict the chosen value more often.\n",
    "            #The loop is also broken\n",
    "            if reward==100:\n",
    "                expected_out = training_board(innum1,outnum1)\n",
    "                #print('\\n\\n\\n', expected_out)\n",
    "                expected_out[0][move_choice] = reward\n",
    "                network.fit(innum1.reshape(1,9), expected_out.reshape(1,9), batch_size=1, nb_epoch=1, verbose=0)\n",
    "                play = False\n",
    "            \n",
    "            #This second part contains the random player's turn\n",
    "            if play and j<4:\n",
    "                #Random player chooses a spot, it is put in the board and it is determined if it has won.\n",
    "                rand_spot = randChoice(board)\n",
    "                reward = rewardFunction(board, int(rand_spot), x=False)\n",
    "                board[rand_spot] = -1\n",
    "                \n",
    "                #If the random player wins, the ANN is trained with a negative value on the move it chose last,\n",
    "                #since that allowed a loss. Loop is broken\n",
    "                if reward == -100:\n",
    "                    expected_out = training_board(innum1,outnum1)\n",
    "                    #print('\\n\\n\\n', expected_out)\n",
    "                    expected_out[0][move_choice] = reward\n",
    "                    network.fit(innum1.reshape(1,9), expected_out.reshape(1,9), batch_size=1, nb_epoch=1, verbose=0)\n",
    "                    play = False\n",
    "                \n",
    "                #If it is not a win, the ANN is trained based on the highest valued next available state.\n",
    "                else:\n",
    "                    innum2 = board\n",
    "                    outnum2 = network.predict(innum2.reshape(1,9), batch_size=1)\n",
    "                    maxQ = max(outnum2[0])\n",
    "                    expected_out = training_board(innum1,outnum1)\n",
    "                    if innum1[move_choice]==0:\n",
    "                        expected_out[0][move_choice] = gamma*maxQ\n",
    "                    #print(innum1, '\\n\\n', expected_out)\n",
    "                    network.fit(innum1.reshape(1,9), expected_out.reshape(1,9), batch_size=1, nb_epoch=1, verbose=0)\n",
    "            j+=1\n",
    "        \n",
    "        #This statement drops ep each iteration until it reaches 0.1\n",
    "        if ep > 0.1:\n",
    "            ep -= 1/games\n",
    "            \n",
    "def print_board(board):\n",
    "    new_board = [0,0,0,0,0,0,0,0,0]\n",
    "    for i in range(9):\n",
    "        if board[i] == 0:\n",
    "            new_board[i] = '.'\n",
    "        if board[i] == 1:\n",
    "            new_board[i] = 'X'\n",
    "        if board[i] == -1:\n",
    "            new_board[i] = 'O'\n",
    "    print(new_board[0], new_board[1], new_board[2])\n",
    "    print(new_board[3], new_board[4], new_board[5])\n",
    "    print(new_board[6], new_board[7], new_board[8], '\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "def play(games):\n",
    "    for i in range(games):\n",
    "        board = np.zeros((9,1))\n",
    "        play = True #if someone wins, the loop is broken\n",
    "        j = 0 # this variable is used to ensure only nine total moves\n",
    "        while play==True and j < 5:\n",
    "            innum1 = board\n",
    "            outnum1 = network.predict(innum1.reshape(1,9), batch_size=1)\n",
    "            move_choice = np.argmax(outnum1)\n",
    "            reward = rewardFunction(innum1, move_choice)\n",
    "            print(\"AI's Move...\")\n",
    "            board[move_choice] = 1\n",
    "            print_board(board)\n",
    "            if reward==1:\n",
    "                print('AI wins!!!')\n",
    "                play = False\n",
    "                    \n",
    "            if play and j<4:\n",
    "                rand_spot = randChoice(board)\n",
    "                reward = rewardFunction(board, int(rand_spot), x=False)\n",
    "                print(\"Random's Move...\")\n",
    "                board[rand_spot] = -1\n",
    "                print_board(board)\n",
    "                if reward == -1:\n",
    "                    print('Player wins!!!')\n",
    "                    play = False\n",
    "            j+=1\n",
    "    \n",
    "\n",
    "########################################################################################################    \n",
    "            \n",
    "games = 10000\n",
    "gamma = 0.99\n",
    "ep = 0.5\n",
    "start_time = time.time()\n",
    "trainNetwork(games, gamma, ep)\n",
    "end_time = time.time()\n",
    "print('Training Time: ', end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI's Move...\n",
      ". . .\n",
      ". . .\n",
      ". X . \n",
      "\n",
      "Random's Move...\n",
      ". . .\n",
      ". O .\n",
      ". X . \n",
      "\n",
      "AI's Move...\n",
      ". . .\n",
      "X O .\n",
      ". X . \n",
      "\n",
      "Random's Move...\n",
      "O . .\n",
      "X O .\n",
      ". X . \n",
      "\n",
      "AI's Move...\n",
      "O . .\n",
      "X O .\n",
      ". X . \n",
      "\n",
      "Random's Move...\n",
      "O . .\n",
      "X O .\n",
      ". X O \n",
      "\n",
      "AI's Move...\n",
      "O . .\n",
      "X X .\n",
      ". X O \n",
      "\n",
      "Random's Move...\n",
      "O O .\n",
      "X X .\n",
      ". X O \n",
      "\n",
      "AI's Move...\n",
      "O O .\n",
      "X X X\n",
      ". X O \n",
      "\n"
     ]
    }
   ],
   "source": [
    "play(1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
