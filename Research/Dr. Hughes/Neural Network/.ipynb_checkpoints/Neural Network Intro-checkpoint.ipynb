{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons\n",
    "\n",
    "Perceptrons take several binary inputs ($x_1, x_2, x_3, ...$) and produces a single binary output ($y$). Perceptrons are analogous to how humans make decisions based on our given factors, hence is the reason why we compare them to a human neuron. \n",
    "\n",
    "![title](resources/percep1.png)\n",
    "\n",
    "For example, let $x_1, x_2, x_3$ represent the factors we take into account when asking someone out on a date. Think of the 3 most important factors you take into account when going on a date. One might be how attractive they are, so let $x_1$ represent the attractiveness of the date. Another might be their personality, $x_2$. Maybe you really like a funny and humorous person so let humor be $x_3$. So out of these 3 qualities you look for in a date, let's say you will go on this date if they meet at least 2 of the 3 factors. In this case, 2 will then be our _threshhold value_. Let $y = 0$ mean not going and $y = 1$ mean going, and we have:\n",
    "\n",
    "\\begin{align}\n",
    "y & = \n",
    "\\left\\{\n",
    "    \\begin{array}{11}\n",
    "        0 & \\quad \\text{if  } x_1 + x_2 + x_3 < 2 \\\\\n",
    "        1 & \\quad \\text{if  } x_1 + x_2 + x_3 \\geq 2\n",
    "    \\end{array}\n",
    "\\right.\n",
    "\\end{align}\n",
    "\n",
    "However, you might think that certain qualities are more important than others when considering a date. Say you value someone with a good humor the most, then their looks, then their personality. This is where we introduce _weights_. These are scalars that express the importance of our respective inputs. So let's say you give a weight value of 8 for looks, $w_1 = 8$, 5 for personality, $w_2 = 5$, and 10 for humor, $w_3 = 10$, and you will only go on the date if the total value is over 14. In this case 14 will then be our threshhold value. So then, we will have:\n",
    "\n",
    "\\begin{align}\n",
    "y & = \n",
    "\\left\\{\n",
    "    \\begin{array}{11}\n",
    "        0 & \\quad w_1x_1 + w_2x_2 + w_3x_3 = 8x_1 + 5x_2 + 10x_3 \\leq 14 \\\\\n",
    "        1 & \\quad w_1x_1 + w_2x_2 + w_3x_3 = 8x_1 + 5x_2 + 10x_3 > 14\n",
    "    \\end{array}\n",
    "\\right.\n",
    "\\end{align}\n",
    "\n",
    "So in this case, you will go on the date if they are funny and good looking or funny and have a good personality, but you won't if they just have good looks and personality. \n",
    "\n",
    "In general, if we have our inputs $x_1, x_2, x_3,$ and their respective weigts $w_1, w_2, w_3, ...$, we add them together, which we will represent as $w \\cdot x \\equiv \\sum_{j} w_j x_j$ to see if they are greater than our threshhold value, which we will now call the _bias_, $b \\equiv \\text{-threshhold}$. So then, our output value can be written as:\n",
    "\n",
    "\\begin{align}\n",
    "y & = \n",
    "\\left\\{\n",
    "    \\begin{array}{11}\n",
    "        0 & \\quad \\text{if  } w \\cdot x + b \\leq 0\\\\\n",
    "        1 & \\quad \\text{if  } w \\cdot x + b > 0\n",
    "    \\end{array}\n",
    "\\right.\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid Neurons\n",
    "\n",
    "As you could probably see in the date example, perceptrons are very limited in their decision making. For example, what if your date was really cute and had a great personality, but they were only _kinda funny_? Would you then go on the date? Well, using the perceptron algorithm, our $x_i$ can only be 0 or 1, so it's not very good at measuring things that are inbetween. This is why we introduce the Sigmoid neurons. Unlike the perceptrons, the inputs and the outputs of the Sigmoid neurons can be anything between 0 and 1.\n",
    "\n",
    "\\begin{align}\n",
    "x_i, y \\in (0, 1)\n",
    "\\end{align}\n",
    "\n",
    "This way, if your date was really cute and had a great personality, but only moderately funny, you could give them $x_1 = 0.9, x_2 = 0.8, x_3 = 0.4$, and you would get:\n",
    "\n",
    "\\begin{align}\n",
    "w \\cdot x + b = 8(0.9) + 5(0.8) + 10(0.4) - 14 = 1.2 > 0\n",
    "\\end{align}\n",
    "\n",
    "which would result in the decision to go on the date! This would not have happend using the perceptron algorithm.\n",
    "\n",
    "As mentioned before, the output of the sigmoid neuron is also between 0 and 1. This value is created by using the sigmoid function defined as followed:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma (z) \\equiv \\frac{1}{1+e^{-z}}, \\quad z = w \\cdot x + b\n",
    "\\end{align}\n",
    "\n",
    "This function shows the following:\n",
    "\n",
    "\\begin{align}\n",
    "z & = w \\cdot x + b \\rightarrow \\infty \\quad \\Rightarrow \\quad e^{-z} \\rightarrow 0 \\quad \\Rightarrow \\quad \\sigma (z) \\rightarrow 1 \\\\\n",
    "z & = w \\cdot x + b \\rightarrow -\\infty \\quad \\Rightarrow \\quad e^{-z} \\rightarrow \\infty \\quad \\Rightarrow \\quad \\sigma (z) \\rightarrow 0\n",
    "\\end{align}\n",
    "\n",
    "So, as $z$ grows very positive, or very negative, the sigmoid function will resemble the perceptron model.\n",
    "\n",
    "Heres the code and a graphical representation to compare the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgEAAAEWCAYAAAD/3UTfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGpQlQJKkQVkCJEka1HdGL/VDskGtpwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1121b6ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def perceptron(z):\n",
    "    if z > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "z = np.linspace(-7, 7, 1000)\n",
    "y = [perceptron(x) for x in z]\n",
    "plt.plot(z, y, 'b', label=\"Perceptron\")\n",
    "plt.plot(z, sigmoid(z), 'r', label=\"Sigmoid\")\n",
    "plt.plot(z, sigmoid_prime(z), 'g', label = \"Sigmoid Prime\")\n",
    "plt.axis([-6,6,-0.1,1.1])\n",
    "plt.title(\"Sigmoid function and Perceptron model\")\n",
    "plt.legend(bbox_to_anchor=(1.1, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel(\"Z\")\n",
    "plt.grid(b=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Structure\n",
    "\n",
    "\n",
    "\n",
    "## Visual Representation\n",
    "\n",
    "![title](resources/NeuralNetworkDiagram.png)\n",
    "\n",
    "## Mathematical Representation\n",
    "\n",
    "## Python Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.rand(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y,x)\n",
    "                        for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if \"a\" is input. \"\"\"\n",
    "        for b,w in zip(self.biases, self.weights):\n",
    "            print(str(a))\n",
    "            print(str(w))\n",
    "            print(str(b))\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "            print(str(a))\n",
    "\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The \"training_data\" is a list of tuples\n",
    "        \"(x, y)\" representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If \"test_data\" is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "        \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\"\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw \n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb \n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y.reshape(10,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 3\n",
      "Biases: [array([[ 0.63667552],\n",
      "       [ 0.46658098],\n",
      "       [ 0.07769445]]), array([[ 0.73956167]])]\n",
      "Weights: [array([[ 0.42649264,  1.25226602],\n",
      "       [ 0.28317665,  2.13355075],\n",
      "       [ 1.48022083, -0.46726286]]), array([[ 0.63632596, -0.54864129,  0.84622838]])]\n"
     ]
    }
   ],
   "source": [
    "n = Network([2,3,1])\n",
    "print(\"Layers: \" + str(n.num_layers))\n",
    "print(\"Biases: \" + str(n.biases))\n",
    "print(\"Weights: \" + str(n.weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.63667552]\n",
      " [ 0.46658098]\n",
      " [ 0.07769445]]\n",
      "[[ 0.42649264  1.25226602]\n",
      " [ 0.28317665  2.13355075]\n",
      " [ 1.48022083 -0.46726286]]\n",
      "[[ 0.99849691  0.99994096  0.84916484]\n",
      " [ 0.9982187   0.99993001  0.82606204]\n",
      " [ 0.99737421  0.99989675  0.76297953]]\n",
      "[[ 0.73956167]]\n",
      "[[ 0.63632596 -0.54864129  0.84622838]]\n",
      "[[ 0.84174554  0.84202703  0.81341438]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.84174554,  0.84202703,  0.81341438]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.feedforward([2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (60000, 10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "X_train.shape, y_train.shape, Y_train.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = []\n",
    "test = []\n",
    "for i in range(len(X_train)):\n",
    "    train.append(X_train[i].flatten())\n",
    "for i in range(len(X_test)):\n",
    "    test.append(X_test[i].flatten())\n",
    "X_train = np.asarray(train)\n",
    "X_test = np.asarray(test)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "        126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "        253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "        253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "        253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "        253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "        183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "        229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "        221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "        213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "        219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "        226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0], dtype=uint8),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "for x,y in zip(X_train, Y_train):\n",
    "    train_data.append((x,y))\n",
    "for x, y in zip(X_test, Y_test):\n",
    "    test_data.append((x,y))\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,  35, 186, 254, 254, 255,\n",
      "       254, 186,  72,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,  96, 232, 253, 253,\n",
      "       253, 254, 253, 253, 232,  95,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 135, 224, 253,\n",
      "       253, 253, 253, 175, 241, 253, 253, 185,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  37, 248,\n",
      "       254, 253, 253, 216, 101,   0,  24, 211, 253, 200,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "        40, 253, 254, 253, 192,   5,   0,   0,   0, 175, 253, 155,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0, 114, 255, 226,  98,   0,   0,   0,  43, 254, 254,\n",
      "       133,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0, 122, 132,   0,   0,   0,   0, 125,\n",
      "       253, 250,  54,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,  30,   0,   0,   0,\n",
      "         0, 214, 253, 146,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,  32, 235, 253, 117,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0, 209, 253, 222,  34,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,  83, 255, 254, 116,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,  13,  65, 185, 214, 244, 254, 243,  42,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0, 186, 253, 253, 253, 253, 254, 247, 108,\n",
      "         6,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,  68, 254, 253, 253, 253, 253, 254,\n",
      "       253, 253, 183,  84,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0, 195, 254, 253, 253, 253,\n",
      "       253, 254, 253, 253, 253, 230,  38,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,  40, 254, 255, 254,\n",
      "       254, 207,  98,  99, 147, 254, 254, 254, 196,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  40, 253,\n",
      "       254, 251, 173,  18,   0,   0,   3, 179, 253, 253, 222,  18,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "        40, 253, 254, 165,   0,   0,   0,   0,   0,  24, 240, 253, 254,\n",
      "        39,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,  18, 191, 131,  10,   0,   0,   0,   0,   0,   0, 181,\n",
      "       253, 244,  33,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0, 100, 253, 120,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "         0,   0,   0,   0], dtype=uint8), array([[  0.00000000e+000,   1.52982243e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.96445701e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   9.19495419e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   1.49983988e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.96374907e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   9.01474494e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   1.37815747e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.96056097e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   8.28337625e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   1.42138402e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.96175578e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   8.54318819e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   1.49144649e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.96354580e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   8.96429669e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   1.69660143e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.96793975e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.01973746e-150,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   1.69605726e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.96792950e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.01941039e-150,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   7.44029898e-091,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.92719229e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   4.47197053e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   1.62270167e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.96648458e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   9.75320221e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   1.48559722e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.96340280e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   8.92913981e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   1.40590071e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.96133623e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   8.45012624e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   1.35647216e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.95993300e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   8.15303730e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   8.10435568e-091,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.93311813e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   4.87109993e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   6.95450973e-091,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.92214610e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   4.17998829e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   7.98703581e-091,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.93214238e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   4.80058515e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   8.21654213e-091,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.93402529e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   4.93852927e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   6.91347371e-091,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.92168760e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   4.15532370e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   1.55283719e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.96498196e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   9.33328375e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   9.35686481e-091,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.94201902e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   5.62391697e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   1.19105574e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.95439379e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   7.15880663e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   7.43095525e-091,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.92710141e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   4.46635451e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   1.43865815e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.96221325e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   8.64701385e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   9.87864470e-091,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.94506469e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   5.93753129e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   9.54935164e-091,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.94318111e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   5.73961064e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   1.09470417e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.95039962e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   6.57968828e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   7.40221497e-091,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.92682044e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   4.44908025e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   1.72614852e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.96848681e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.03749666e-150,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   1.33526837e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.95929933e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   8.02559253e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   9.93936389e-091,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.94539846e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   5.97402639e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000],\n",
      "       [  0.00000000e+000,   1.20267157e-090,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          0.00000000e+000,   9.95483228e-001,   1.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   0.00000000e+000,\n",
      "          1.00000000e+000,   0.00000000e+000,   1.00000000e+000,\n",
      "          1.00000000e+000,   1.00000000e+000,   7.22862328e-151,\n",
      "          0.00000000e+000,   1.00000000e+000,   0.00000000e+000]]), array([[  6.46890152e-01,   6.46890152e-01,   1.06679005e-03,\n",
      "          1.06679005e-03,   6.46890152e-01,   1.06679005e-03,\n",
      "          1.06679005e-03,   6.46890152e-01,   1.06679005e-03,\n",
      "          1.06679005e-03,   1.06679005e-03,   1.06679005e-03,\n",
      "          1.06679005e-03,   6.46890152e-01,   1.06679005e-03,\n",
      "          6.46890152e-01,   1.09381210e-03,   1.06679005e-03,\n",
      "          1.06679005e-03,   6.46890152e-01,   6.46890152e-01,\n",
      "          1.06679005e-03,   6.46890152e-01,   1.06679005e-03,\n",
      "          1.06679005e-03,   1.06679005e-03,   6.46890152e-01,\n",
      "          6.46890152e-01,   1.06679005e-03,   6.46890152e-01],\n",
      "       [  5.09915359e-01,   5.09915359e-01,   3.37827489e-01,\n",
      "          3.37827489e-01,   5.09915359e-01,   3.37827489e-01,\n",
      "          3.37827489e-01,   5.09915359e-01,   3.37827489e-01,\n",
      "          3.37827489e-01,   3.37827489e-01,   3.37827489e-01,\n",
      "          3.37827489e-01,   5.09915359e-01,   3.37827489e-01,\n",
      "          5.09915359e-01,   3.38802742e-01,   3.37827489e-01,\n",
      "          3.37827489e-01,   5.09915359e-01,   5.09915359e-01,\n",
      "          3.37827489e-01,   5.09915359e-01,   3.37827489e-01,\n",
      "          3.37827489e-01,   3.37827489e-01,   5.09915359e-01,\n",
      "          5.09915359e-01,   3.37827489e-01,   5.09915359e-01],\n",
      "       [  5.93940649e-01,   5.93940649e-01,   8.05926986e-04,\n",
      "          8.05926986e-04,   5.93940649e-01,   8.05926986e-04,\n",
      "          8.05926986e-04,   5.93940649e-01,   8.05926986e-04,\n",
      "          8.05926986e-04,   8.05926986e-04,   8.05926986e-04,\n",
      "          8.05926986e-04,   5.93940649e-01,   8.05926986e-04,\n",
      "          5.93940649e-01,   8.39093007e-04,   8.05926986e-04,\n",
      "          8.05926986e-04,   5.93940649e-01,   5.93940649e-01,\n",
      "          8.05926986e-04,   5.93940649e-01,   8.05926986e-04,\n",
      "          8.05926986e-04,   8.05926986e-04,   5.93940649e-01,\n",
      "          5.93940649e-01,   8.05926986e-04,   5.93940649e-01],\n",
      "       [  5.10244508e-01,   5.10244508e-01,   9.74754395e-01,\n",
      "          9.74754395e-01,   5.10244508e-01,   9.74754395e-01,\n",
      "          9.74754395e-01,   5.10244508e-01,   9.74754395e-01,\n",
      "          9.74754395e-01,   9.74754395e-01,   9.74754395e-01,\n",
      "          9.74754395e-01,   5.10244508e-01,   9.74754395e-01,\n",
      "          5.10244508e-01,   9.73935345e-01,   9.74754395e-01,\n",
      "          9.74754395e-01,   5.10244508e-01,   5.10244508e-01,\n",
      "          9.74754395e-01,   5.10244508e-01,   9.74754395e-01,\n",
      "          9.74754395e-01,   9.74754395e-01,   5.10244508e-01,\n",
      "          5.10244508e-01,   9.74754395e-01,   5.10244508e-01],\n",
      "       [  5.14651258e-01,   5.14651258e-01,   9.83358075e-01,\n",
      "          9.83358075e-01,   5.14651258e-01,   9.83358075e-01,\n",
      "          9.83358075e-01,   5.14651258e-01,   9.83358075e-01,\n",
      "          9.83358075e-01,   9.83358075e-01,   9.83358075e-01,\n",
      "          9.83358075e-01,   5.14651258e-01,   9.83358075e-01,\n",
      "          5.14651258e-01,   9.83104861e-01,   9.83358075e-01,\n",
      "          9.83358075e-01,   5.14651258e-01,   5.14651258e-01,\n",
      "          9.83358075e-01,   5.14651258e-01,   9.83358075e-01,\n",
      "          9.83358075e-01,   9.83358075e-01,   5.14651258e-01,\n",
      "          5.14651258e-01,   9.83358075e-01,   5.14651258e-01],\n",
      "       [  6.17921963e-01,   6.17921963e-01,   1.13308463e-02,\n",
      "          1.13308463e-02,   6.17921963e-01,   1.13308463e-02,\n",
      "          1.13308463e-02,   6.17921963e-01,   1.13308463e-02,\n",
      "          1.13308463e-02,   1.13308463e-02,   1.13308463e-02,\n",
      "          1.13308463e-02,   6.17921963e-01,   1.13308463e-02,\n",
      "          6.17921963e-01,   1.15922059e-02,   1.13308463e-02,\n",
      "          1.13308463e-02,   6.17921963e-01,   6.17921963e-01,\n",
      "          1.13308463e-02,   6.17921963e-01,   1.13308463e-02,\n",
      "          1.13308463e-02,   1.13308463e-02,   6.17921963e-01,\n",
      "          6.17921963e-01,   1.13308463e-02,   6.17921963e-01],\n",
      "       [  5.31331764e-01,   5.31331764e-01,   3.76084380e-01,\n",
      "          3.76084380e-01,   5.31331764e-01,   3.76084380e-01,\n",
      "          3.76084380e-01,   5.31331764e-01,   3.76084380e-01,\n",
      "          3.76084380e-01,   3.76084380e-01,   3.76084380e-01,\n",
      "          3.76084380e-01,   5.31331764e-01,   3.76084380e-01,\n",
      "          5.31331764e-01,   3.77227284e-01,   3.76084380e-01,\n",
      "          3.76084380e-01,   5.31331764e-01,   5.31331764e-01,\n",
      "          3.76084380e-01,   5.31331764e-01,   3.76084380e-01,\n",
      "          3.76084380e-01,   3.76084380e-01,   5.31331764e-01,\n",
      "          5.31331764e-01,   3.76084380e-01,   5.31331764e-01],\n",
      "       [  5.75211196e-01,   5.75211196e-01,   1.71070157e-01,\n",
      "          1.71070157e-01,   5.75211196e-01,   1.71070157e-01,\n",
      "          1.71070157e-01,   5.75211196e-01,   1.71070157e-01,\n",
      "          1.71070157e-01,   1.71070157e-01,   1.71070157e-01,\n",
      "          1.71070157e-01,   5.75211196e-01,   1.71070157e-01,\n",
      "          5.75211196e-01,   1.72568353e-01,   1.71070157e-01,\n",
      "          1.71070157e-01,   5.75211196e-01,   5.75211196e-01,\n",
      "          1.71070157e-01,   5.75211196e-01,   1.71070157e-01,\n",
      "          1.71070157e-01,   1.71070157e-01,   5.75211196e-01,\n",
      "          5.75211196e-01,   1.71070157e-01,   5.75211196e-01],\n",
      "       [  6.30561100e-01,   6.30561100e-01,   6.26919468e-01,\n",
      "          6.26919468e-01,   6.30561100e-01,   6.26919468e-01,\n",
      "          6.26919468e-01,   6.30561100e-01,   6.26919468e-01,\n",
      "          6.26919468e-01,   6.26919468e-01,   6.26919468e-01,\n",
      "          6.26919468e-01,   6.30561100e-01,   6.26919468e-01,\n",
      "          6.30561100e-01,   6.27856211e-01,   6.26919468e-01,\n",
      "          6.26919468e-01,   6.30561100e-01,   6.30561100e-01,\n",
      "          6.26919468e-01,   6.30561100e-01,   6.26919468e-01,\n",
      "          6.26919468e-01,   6.26919468e-01,   6.30561100e-01,\n",
      "          6.30561100e-01,   6.26919468e-01,   6.30561100e-01],\n",
      "       [  6.51076073e-01,   6.51076073e-01,   1.06391263e-01,\n",
      "          1.06391263e-01,   6.51076073e-01,   1.06391263e-01,\n",
      "          1.06391263e-01,   6.51076073e-01,   1.06391263e-01,\n",
      "          1.06391263e-01,   1.06391263e-01,   1.06391263e-01,\n",
      "          1.06391263e-01,   6.51076073e-01,   1.06391263e-01,\n",
      "          6.51076073e-01,   1.08797696e-01,   1.06391263e-01,\n",
      "          1.06391263e-01,   6.51076073e-01,   6.51076073e-01,\n",
      "          1.06391263e-01,   6.51076073e-01,   1.06391263e-01,\n",
      "          1.06391263e-01,   1.06391263e-01,   6.51076073e-01,\n",
      "          6.51076073e-01,   1.06391263e-01,   6.51076073e-01]])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (30,30) and (784,) not aligned: 30 (dim 1) != 784 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-dd11cdec9778>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-53262fc96952>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(self, training_data, epochs, mini_batch_size, eta, test_data)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 for k in range(0, n, mini_batch_size)]\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmini_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 print(\"Epoch {0}: {1} / {2}\".format(\n",
      "\u001b[0;32m<ipython-input-28-53262fc96952>\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[0;34m(self, mini_batch, eta)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mnabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdelta_nabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mnabla_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdnb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mnabla_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnw\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdnw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdnw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnabla_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-53262fc96952>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mnabla_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mnabla_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnabla_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnabla_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (30,30) and (784,) not aligned: 30 (dim 1) != 784 (dim 0)"
     ]
    }
   ],
   "source": [
    "net = Network([784, 30,10])\n",
    "net.SGD(train_data, 30, 10, 3.0, test_data = test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self):\n",
    "        self.bias = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyNetwork(object):\n",
    "    def __init__(self, layers):\n",
    "        self.num_layers = len(layers)\n",
    "        self.layers = layers\n",
    "        self.neurons = self.create_neurons()\n",
    "        self.weights = \n",
    "        \n",
    "    def create_neurons(self):\n",
    "        neurons = []\n",
    "        for n in self.layers:\n",
    "            current_layer = []\n",
    "            for i in range(n):\n",
    "                node = Neuron()\n",
    "                current_layer.append(node)\n",
    "            neurons.append(current_layer)\n",
    "        return neurons\n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
